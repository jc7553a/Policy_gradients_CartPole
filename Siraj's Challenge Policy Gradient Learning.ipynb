{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Siraj's Challenge: Policy Gradient Learning with Cart Pole V0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIMONINI Thomas <br>\n",
    "<a href=\"https://github.com/simoninithomas\"> Github </a><br>\n",
    "<a href=\"https://www.simoninithomas.com\"> Website </a><br>\n",
    "<a href=\"mailto:hello@simoninithomas.com\"> My email </a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge of the week was: <b>solving a simple game using policy gradients (other than pong).</b>\n",
    "I've chosen CartPole v1.0 because that's a basic game and there is a ton of documentations/tutorials about that kind of game. \n",
    "\n",
    "<h3> Goal </h3>\n",
    "CartPole-v0 defines \"solving\" as <b>getting average reward of 195.0 over 100 consecutive trials. </b>\n",
    "\n",
    "<br>\n",
    "I've chosen to use jupyter notebook because <b> it's much better to understand the code </b> in this notebook, I will <b> try to explain all the parts of the code </b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Some interesting discoveries </h3>\n",
    "<ul>\n",
    "    <li>Using ELU instead ReLU <b> lead to better results </b> </li>\n",
    "    <li> Using RMSProp as optimizer <b> lead to better results </b></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Cart Pole V0 </h3>\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*G_whtIrY9fGlw3It6HFfhA.gif\" alt=\"Cart Pole game\" />\n",
    "\n",
    "4 kinds of information given by the state:\n",
    "<ul>\n",
    "    <li>Position of the cart</li>\n",
    "    <li> Velocity of the cart </li>\n",
    "    <li> Position of the pole </li>\n",
    "    <li> Velocity of the pole </li>\n",
    "</ul>\n",
    "<br>\n",
    "An agent can push the cart:\n",
    "<ul>\n",
    "    <li> 0: left </li>\n",
    "    <li> 1: right </ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was made possible thanks these 2 fantastic resources:\n",
    "<ul>\n",
    "    <li> <a href=\"https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724\">Simple Reinforcement Learning with Tensorflow: Part 2 - Policy-based Agents </a> : this article helps me to define a part of the architecture and helps me a lot for the training part.</li>\n",
    "    \n",
    "   \n",
    "  <li> <a href=\"https://gist.github.com/shanest/535acf4c62ee2a71da498281c2dfc4f4\" >Policy gradients for reinforcement learning in TensorFlow</a></li>\n",
    "  </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-13 17:33:10,592] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Watch the simulation\n",
    "env.reset()\n",
    "rewards = []\n",
    "\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    \n",
    "    # Take a random action\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_size = 4 # 4 informations given by state\n",
    "action_size = 2 # 2 actions possible: left / right\n",
    "hidden_size = 64 # Hidden neurons\n",
    "\n",
    "learning_rate = 0.001 \n",
    "gamma = 0.99 #Discount rate\n",
    "\n",
    "train_episodes = 5000 # An episode is a game\n",
    "max_steps = 900 # Max steps per episode\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build our Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/nn.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Originally taken from, <a href=\"https://www.youtube.com/watch?v=pN7ETkOizGM\">Siraj's Solving the basic game of Pong video </a> modified with my exceptional skills in paint </i>ðŸ˜‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class PGAgent():\n",
    "    def __init__(self, input_size, action_size, hidden_size, learning_rate, gamma):\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Make the NN\n",
    "        self.inputs = tf.placeholder(tf.float32, \n",
    "                      shape = [None, input_size])\n",
    "                              \n",
    "        # Using ELU is much better than using ReLU\n",
    "        self.hidden_layer_1 = tf.contrib.layers.fully_connected(inputs = self.inputs,\n",
    "                                                  num_outputs = hidden_size,\n",
    "                                                  activation_fn = tf.nn.elu,\n",
    "                                                  weights_initializer = tf.random_normal_initializer())\n",
    "\n",
    "        self.output_layer = tf.contrib.layers.fully_connected(inputs = self.hidden_layer_1,\n",
    "                                                         num_outputs = action_size,\n",
    "                                                 activation_fn = tf.nn.softmax)\n",
    "        \n",
    "        # Log prob output\n",
    "        self.output_log_prob = tf.log(self.output_layer)\n",
    "        \n",
    "        \n",
    "        ### LOSS Function : feed the reward and chosen action in the DNN\n",
    "        # Taken from this implementation https://gist.github.com/shanest/535acf4c62ee2a71da498281c2dfc4f4\n",
    "        \n",
    "        self.actions = tf.placeholder(tf.int32, shape = [None])\n",
    "        self.rewards = tf.placeholder(tf.float32, shape = [None])\n",
    "        \n",
    "        # Get log probability of actions from episode : \n",
    "        self.indices = tf.range(0, tf.shape(self.output_log_prob)[0]) * tf.shape(self.output_log_prob)[1] + self.actions\n",
    "        \n",
    "        self.actions_probability = tf.gather(tf.reshape(self.output_layer, [-1]), self.indices)\n",
    "        \n",
    "        self.loss = -tf.reduce_mean(tf.log(self.actions_probability) * self.rewards)\n",
    "        \n",
    "  \n",
    "\n",
    "        #  Collect some gradients after some training episodes outside the graph and then apply them.\n",
    "        # Not implemented by me, taken from https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.mtwpvfi8b\n",
    "        tvars = tf.trainable_variables()\n",
    "        self.gradient_holders = []\n",
    "        for idx,var in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32, name=str(idx)+ '_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss,tvars)\n",
    "        \n",
    "        \n",
    "        ### OPTIMIZER\n",
    "        \n",
    "        # Better to use RMSProp\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders,tvars))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our advantage function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>What we must understand here is that immediate rewards <b>are more important than delayed rewards.</b>\n",
    "</p>\n",
    "<p> That's why we use gamma as a discount factor </p>\n",
    "<img src=\"assets/discountreward.png\" alt=\"Discount reward\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why ? Because <b>delayed rewards have less impact</b>: imagine you screw up at step 5 (the bar is too leaning) we don't care of rewards after that because you will lose that's why the reward is more and more discounted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/d1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/d2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Originally taken from, <a href=\"https://www.youtube.com/watch?v=tqrcjHuNdmQ\">DQN Bootcamp Lecture: Core Lecture 4b Pong from Pixels -- Andrej Karpathy </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Weight rewards differently : weight immediate rewards higher than delayed reward\n",
    "\n",
    "def discount_rewards(r):\n",
    "    # Init discount reward matrix\n",
    "    discounted_reward= np.zeros_like(r) \n",
    "    \n",
    "    # Running_add: store sum of reward\n",
    "    running_add = 0\n",
    "    \n",
    "    # Foreach rewards\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        \n",
    "        running_add = running_add * gamma + r[t] # sum * y (gamma) + reward\n",
    "        discounted_reward[t] = running_add\n",
    "    return discounted_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that:\n",
    "<ul>\n",
    "    <li> A positive advantage --> make the action <b>more likely to happen in the future</b>, at that state </li>\n",
    "    <li> A negative advantage --> make the action <b>less likely to happen in the future</b>, at that state</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 23.0\n",
      "Episode: 100 Total reward: 46.47\n",
      "Episode: 200 Total reward: 49.83\n",
      "Episode: 300 Total reward: 57.72\n",
      "Episode: 400 Total reward: 62.65\n",
      "Episode: 500 Total reward: 89.78\n",
      "Episode: 600 Total reward: 105.59\n",
      "Episode: 700 Total reward: 133.91\n",
      "Episode: 800 Total reward: 142.04\n",
      "Episode: 900 Total reward: 151.3\n",
      "Episode: 1000 Total reward: 159.59\n",
      "Episode: 1100 Total reward: 159.25\n",
      "Episode: 1200 Total reward: 168.02\n",
      "Episode: 1300 Total reward: 153.65\n",
      "Episode: 1400 Total reward: 166.91\n",
      "Episode: 1500 Total reward: 175.99\n",
      "Episode: 1600 Total reward: 180.67\n",
      "Episode: 1700 Total reward: 182.99\n",
      "Episode: 1800 Total reward: 183.64\n",
      "Episode: 1900 Total reward: 191.79\n",
      "Episode: 2000 Total reward: 184.14\n",
      "Episode: 2100 Total reward: 180.97\n",
      "Episode: 2200 Total reward: 190.58\n",
      "Episode: 2300 Total reward: 190.56\n",
      "Episode: 2400 Total reward: 185.67\n",
      "Episode: 2500 Total reward: 192.32\n",
      "Episode: 2600 Total reward: 191.32\n",
      "Episode: 2700 Total reward: 192.74\n",
      "Episode: 2800 Total reward: 190.67\n",
      "Episode: 2900 Total reward: 195.43\n",
      "Episode: 3000 Total reward: 194.22\n",
      "Episode: 3100 Total reward: 193.42\n",
      "Episode: 3200 Total reward: 192.43\n",
      "Episode: 3300 Total reward: 191.27\n",
      "Episode: 3400 Total reward: 184.21\n",
      "Episode: 3500 Total reward: 186.51\n",
      "Episode: 3600 Total reward: 188.21\n",
      "Episode: 3700 Total reward: 191.81\n",
      "Episode: 3800 Total reward: 195.92\n",
      "Episode: 3900 Total reward: 181.83\n",
      "Episode: 4000 Total reward: 192.3\n",
      "Episode: 4100 Total reward: 189.15\n",
      "Episode: 4200 Total reward: 184.76\n",
      "Episode: 4300 Total reward: 182.34\n",
      "Episode: 4400 Total reward: 161.49\n",
      "Episode: 4500 Total reward: 174.49\n",
      "Episode: 4600 Total reward: 168.96\n",
      "Episode: 4700 Total reward: 174.41\n",
      "Episode: 4800 Total reward: 184.35\n",
      "Episode: 4900 Total reward: 192.04\n"
     ]
    }
   ],
   "source": [
    "# Clear the graph\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "agent = PGAgent(input_size, action_size, hidden_size, learning_rate, gamma)\n",
    "\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    nb_episodes = 0\n",
    "    \n",
    "    # Define total_rewards and total_length\n",
    "    total_reward = []\n",
    "    total_length = []\n",
    "    \n",
    "    # Not my implementation: \n",
    "    gradBuffer = sess.run(tf.trainable_variables())\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "        \n",
    "    \n",
    "    # While we have episodes to train\n",
    "    while nb_episodes < train_episodes:\n",
    "        state = env.reset()\n",
    "        running_reward = 0\n",
    "        episode_history = [] # Init the array that keep track the history in an episode\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            #Probabilistically pick an action given our network outputs.\n",
    "            # Not my implementation: taken from Udacity Q-learning quart https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb \n",
    "            action_distribution = sess.run(agent.output_layer ,feed_dict={agent.inputs:[state]})\n",
    "            action = np.random.choice(action_distribution[0],p=action_distribution[0])\n",
    "            action = np.argmax(action_distribution == action)\n",
    "            \n",
    "            state_1, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Append this step in the history of the episode\n",
    "            episode_history.append([state, action, reward, state_1])\n",
    "            \n",
    "            # Now we are in this state (state is now state 1)\n",
    "            state = state_1\n",
    "            \n",
    "            running_reward += reward\n",
    "            \n",
    "            if done == True:\n",
    "                # Update the network\n",
    "                episode_history = np.array(episode_history)\n",
    "                episode_history[:,2] = discount_rewards(episode_history[:,2])\n",
    "                feed_dict={agent.rewards:episode_history[:,2],\n",
    "                        agent.actions:episode_history[:,1],agent.inputs:np.vstack(episode_history[:,0])}\n",
    "                grads = sess.run(agent.gradients, feed_dict=feed_dict)\n",
    "                \n",
    "                \n",
    "                for idx,grad in enumerate(grads):\n",
    "                    gradBuffer[idx] += grad\n",
    "\n",
    "                if nb_episodes % batch_size == 0 and nb_episodes != 0:\n",
    "                    feed_dict= dictionary = dict(zip(agent.gradient_holders, gradBuffer))\n",
    "                    _ = sess.run(agent.update_batch, feed_dict=feed_dict)\n",
    "                    for ix,grad in enumerate(gradBuffer):\n",
    "                        gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                #(running_reward))\n",
    "                total_reward.append(running_reward)\n",
    "                total_length.append(step)\n",
    "                break\n",
    "                \n",
    "        # For each 100 episodes\n",
    "        if nb_episodes % 100 == 0:\n",
    "            print(\"Episode: {}\".format(nb_episodes),\n",
    "                    \"Total reward: {}\".format(np.mean(total_reward[-100:])))\n",
    "        nb_episodes += 1\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/cartPoleGame.ckpt\")\n",
    "        \n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Play the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Let see our agent playing the game </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_episodes = 10\n",
    "test_max_steps = 400\n",
    "env.reset()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    for episode in range(1, test_episodes):\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            env.render() \n",
    "            \n",
    "        \n",
    "            \n",
    "            #Probabilistically pick an action given our network outputs.\n",
    "            # Not my implementation: taken from Udacity Q-learning quart https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb \n",
    "            action_distribution = sess.run(agent.output_layer ,feed_dict={agent.inputs:[state]})\n",
    "            action = np.random.choice(action_distribution[0],p=action_distribution[0])\n",
    "            action = np.argmax(action_distribution == action)\n",
    "            \n",
    "            state_1, reward, done, info = env.step(action)\n",
    "           \n",
    "            \n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                state = state_1 # Next state\n",
    "                t += 1\n",
    "                \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
